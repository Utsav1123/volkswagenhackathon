from keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn.metrics as sk_met
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import pickle
import tkinter as tk
from tkinter import filedialog
import cv2 as cv
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy
from sklearn.model_selection import train_test_split

df = pd.read_csv('en-fr.csv')

input_texts = df['text'].values
labels = df['label'].values

tokenizer_input = Tokenizer()
tokenizer_input.fit_on_texts(input_texts)

input_sequences = tokenizer_input.texts_to_sequences(input_texts)

max_sequence_length = 50  
input_vocab_size = len(tokenizer_input.word_index) + 1

input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')

x_train, x_test, y_train, y_test = train_test_split(input_sequences, labels, test_size=0.2, random_state=42)

def translate():
    input= tf.keras.Input()
    e1= tf.keas.layers.Embedding(input_dim=input_vocab_size,output_dim=500,mask_zero=True)(input)
    lstm1= tf.keras.layers.LSTM(input=input_vocab_size)(e1)
    relu= tf.keras.layers.TimeDistributed(Dense(1000, activation='relu'))(lstm1)
    dropout= tf.keras.layers.Dropout(0.3)(relu)
    softmaxout= tf.keras.layers.TimeDistributed(Dense(6, activation='softmax'))(dropout)
    model=tf.keras.Model(input,softmaxout)
    return model

model=translate() 
model.compile(optimizer='Adam',metrics=['accuracy'])
model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=5)